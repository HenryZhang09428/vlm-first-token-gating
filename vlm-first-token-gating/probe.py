# -*- coding: utf-8 -*-
"""probe.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y7MFe052MdO_XNEvuohO-WBJ4zuAq6ck
"""

import torch
import torch.nn as nn
import numpy as np
from sklearn.metrics import f1_score, accuracy_score, roc_auc_score, average_precision_score, confusion_matrix
from sklearn.model_selection import StratifiedShuffleSplit
from data_loader import MemmapDataset
from utils import set_seed

device = "cuda" if torch.cuda.is_available() else "cpu"

class LinearProbeLN(nn.Module):
    def __init__(self, d, n):
        super().__init__()
        self.ln = nn.LayerNorm(d, elementwise_affine=False)
        self.fc = nn.Linear(d, n)
    def forward(self, x):
        return self.fc(self.ln(x))

def make_disjoint_splits(y, val_ratio=0.2, test_ratio=0.2, seed=42):
    y = np.asarray(y)
    sss1 = StratifiedShuffleSplit(n_splits=1, test_size=test_ratio, random_state=seed)
    trainval_idx, test_idx = next(sss1.split(np.zeros(len(y)), y))
    sss2 = StratifiedShuffleSplit(n_splits=1, test_size=val_ratio/(1 - test_ratio), random_state=seed)
    tr_sub, va_sub = next(sss2.split(np.zeros(len(trainval_idx)), y[trainval_idx]))
    return trainval_idx[tr_sub], trainval_idx[va_sub], test_idx

def train_linear_probe(mm_path, labels, train_idx, val_idx, vocab_size, n_classes=2, epochs=8, seed=42):
    set_seed(seed)
    ds = MemmapDataset(mm_path, labels)
    dl_tr = torch.utils.data.DataLoader(torch.utils.data.Subset(ds, train_idx), batch_size=256, shuffle=True)
    dl_va = torch.utils.data.DataLoader(torch.utils.data.Subset(ds, val_idx), batch_size=256, shuffle=False)

    head = LinearProbeLN(vocab_size, n_classes).to(device)
    y_tr = ds.y[train_idx].numpy()
    p1 = max(1, int(y_tr.sum())); p0 = max(1, int(len(y_tr) - y_tr.sum()))
    w = torch.tensor([p1/(p0+p1), p0/(p0+p1)], dtype=torch.float32, device=device)

    opt = torch.optim.AdamW(head.parameters(), lr=1e-2, weight_decay=1e-4)
    criterion = nn.CrossEntropyLoss(weight=w)

    best_f1, best_state = -1.0, None
    for _ in range(epochs):
        head.train()
        for xb, yb in dl_tr:
            xb = torch.nan_to_num(xb.to(device), nan=0.0, posinf=5e3, neginf=-5e3)
            loss = criterion(head(xb), yb.to(device))
            opt.zero_grad(); loss.backward(); opt.step()

        # Validation
        head.eval()
        probs, ys = [], []
        with torch.no_grad():
            for xb, yb in dl_va:
                xb = torch.nan_to_num(xb.to(device), nan=0.0, posinf=5e3, neginf=-5e3)
                p = torch.softmax(head(xb), dim=-1)[:,1].cpu().numpy()
                probs.append(p); ys.append(yb.numpy())
        f1 = f1_score(np.concatenate(ys), (np.concatenate(probs)>=0.5).astype(int))
        if f1 > best_f1: best_f1 = f1; best_state = head.state_dict()

    head.load_state_dict(best_state)
    return head

def evaluate_probe(head, mm_path, labels, test_idx, threshold=0.5):
    ds = MemmapDataset(mm_path, labels)
    dl = torch.utils.data.DataLoader(torch.utils.data.Subset(ds, test_idx), batch_size=512, shuffle=False)
    head.eval()
    probs, ys = [], []
    with torch.no_grad():
        for xb, yb in dl:
            xb = torch.nan_to_num(xb.to(device), nan=0.0, posinf=5e3, neginf=-5e3)
            p = torch.softmax(head(xb), dim=-1)[:,1].cpu().numpy()
            probs.append(p); ys.append(yb.numpy())

    probs = np.concatenate(probs)
    ys = np.concatenate(ys)
    pred = (probs >= threshold).astype(int)

    return {
        "acc": accuracy_score(ys, pred),
        "f1": f1_score(ys, pred),
        "auc": roc_auc_score(ys, probs) if len(np.unique(ys))>1 else float('nan'),
        "probs": probs,
        "ys": ys,
        "test_idx": test_idx
    }