# -*- coding: utf-8 -*-
"""gating.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y7MFe052MdO_XNEvuohO-WBJ4zuAq6ck
"""

import torch
import numpy as np
from configs import SystemPrompts
from utils import blank_img_or_open
from model_core import chat_first_token_logits, chat_generate, device

def calculate_uncertainty(logits, processor):
    # Logits Cleaning
    logits_clean = np.nan_to_num(logits.astype(np.float32), nan=-1e10, posinf=5e3, neginf=-5e3)
    probs = torch.softmax(torch.from_numpy(logits_clean), dim=-1).numpy()

    # 1. Normalized Entropy
    entropy = -np.sum(probs * np.log(probs + 1e-10))
    norm_entropy = entropy / np.log(len(probs))

    # 2. Top-2 Margin
    top2 = np.partition(probs, -2)[-2:]
    margin = 1.0 - (top2[1] - top2[0])

    # 3. Yes/No Probability Gap
    yes_ids = processor.tokenizer.encode("yes", add_special_tokens=False)
    no_ids = processor.tokenizer.encode("no", add_special_tokens=False)
    p_yes = sum(probs[t] for t in yes_ids if t < len(probs))
    p_no = sum(probs[t] for t in no_ids if t < len(probs))
    yesno_u = 1.0 - abs(p_yes - p_no)

    # Weighted Sum
    return 0.4 * norm_entropy + 0.3 * margin + 0.3 * yesno_u

def gated_generate(model, processor, image_path, question,
                   safety_head=None, safety_threshold=None,
                   unc_cautious=0.15, unc_refuse=0.35, use_safety=True):

    img = blank_img_or_open(image_path)

    # --- 安全门 ---
    p_refuse = None
    if use_safety and safety_head and safety_threshold:
        l1 = chat_first_token_logits(model, processor, img, question, SystemPrompts.SAFETY_TRAIN)
        x1 = torch.from_numpy(np.nan_to_num(l1, nan=0, posinf=5e3, neginf=-5e3)).unsqueeze(0).to(device)
        with torch.no_grad():
            p_refuse = torch.softmax(safety_head(x1.float()), dim=-1)[0,1].item()

        if p_refuse >= safety_threshold:
            txt = chat_generate(model, processor, img, question, SystemPrompts.SAFETY_GEN,
                                "Sorry, I can't help with that. General safety info: ")
            return {"decision": "refuse_safety", "text": txt}

    # --- 不确定性门 ---
    l2 = chat_first_token_logits(model, processor, img, question, SystemPrompts.POPE_TRAIN)
    unc = calculate_uncertainty(l2, processor)

    if unc >= unc_refuse:
        txt = chat_generate(model, processor, img, question, SystemPrompts.CAUTIOUS_GEN,
                            "I cannot answer this with sufficient confidence. ")
        return {"decision": "refuse_uncertainty", "text": txt}
    elif unc >= unc_cautious:
        txt = chat_generate(model, processor, img, question, SystemPrompts.CAUTIOUS_GEN,
                            "I am uncertain; here is a cautious answer: ")
        return {"decision": "cautious", "text": txt}

    # --- 正常 ---
    txt = chat_generate(model, processor, img, question, SystemPrompts.NORMAL_GEN)
    return {"decision": "normal", "text": txt}