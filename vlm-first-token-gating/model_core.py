# -*- coding: utf-8 -*-
"""model_core.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y7MFe052MdO_XNEvuohO-WBJ4zuAq6ck
"""

import torch
import numpy as np
from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor
from utils import blank_image

device = "cuda" if torch.cuda.is_available() else "cpu"

def load_model(model_id):
    maj, _ = torch.cuda.get_device_capability(0) if torch.cuda.is_available() else (0,0)
    dtype = torch.bfloat16 if maj >= 8 else torch.float16

    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
        model_id, torch_dtype=dtype, attn_implementation="sdpa"
    ).to(device)
    processor = AutoProcessor.from_pretrained(model_id)
    model.eval()
    return model, processor

def _process_vision_info(messages):
    images, videos = [], []
    for m in messages:
        c = m.get("content", [])
        if isinstance(c, list):
            for e in c:
                if isinstance(e, dict):
                    if e.get("type") == "image" and e.get("image") is not None:
                        images.append(e["image"])
    return images if images else None, videos if videos else None

def chat_first_token_logits(model, processor, image, question, system_prompt=None, max_new_tokens=1):
    msgs=[]
    if system_prompt: msgs.append({"role":"system","content":system_prompt})
    content=[]
    if image is not None: content.append({"type":"image","image":image})
    content.append({"type":"text","text":question})
    msgs.append({"role":"user","content":content})

    text = processor.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)
    image_inputs, video_inputs = _process_vision_info(msgs)

    inputs = processor(
        text=[text], images=image_inputs, videos=video_inputs, padding=True, return_tensors="pt"
    ).to(model.device)

    with torch.no_grad():
        out = model.generate(
            **inputs, max_new_tokens=max_new_tokens, do_sample=False,
            output_scores=True, return_dict_in_generate=True
        )
    logits = out.scores[0][0].detach().float().cpu().numpy()
    return logits

def chat_generate(model, processor, image, question, system_prompt=None, assistant_prefix=None, max_new_tokens=128):
    msgs=[]
    if system_prompt: msgs.append({"role":"system","content":system_prompt})
    content=[]
    if image is not None: content.append({"type":"image","image":image})
    content.append({"type":"text","text":question})
    msgs.append({"role":"user","content":content})

    if assistant_prefix:
        msgs.append({"role":"assistant","content":[{"type":"text","text":assistant_prefix}]})
        text = processor.apply_chat_template(msgs, tokenize=False, add_generation_prompt=False)
    else:
        text = processor.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)

    image_inputs, _ = _process_vision_info(msgs)
    inputs = processor(text=[text], images=image_inputs, padding=True, return_tensors="pt").to(model.device)

    with torch.no_grad():
        out = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)

    gen = out[0][inputs["input_ids"].shape[-1]:]
    return (assistant_prefix or "") + processor.tokenizer.decode(gen, skip_special_tokens=True).strip()